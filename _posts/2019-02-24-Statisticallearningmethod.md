---
layout: post
category: 机器学习
title:  《统计学习方法》- 笔记
catalog: true
excerpt:  阅读与整理的《统计学习方法》简单笔记
time: 2019.02.24 15:35:00
tags:
- 机器学习

---


# 统计学习方法-笔记

## 统计学习方法相关基础

![5](https://mo-xiaoxi.github.io/img/post/machinelearning/5.png)

![6](https://mo-xiaoxi.github.io/img/post/machinelearning/6.png)

- 关于数据的基本假设:同类数据具有一定的统计规律性.

- 统计学习方法是基于数据构建统计模型,从而对数据进行预测与分析.统计学习由监督学习、非监督学习、半监督学习和强化学习组成.

- 统计学习方法的三要素:模型、策略和算法.

- 监督学习:
  - 从给定的、有限的、用于学习的训练数据集合出发(假设数据是独立同分布产生的).
  - 假设要学习的模型属于某个函数的集合,即假设空间.
  - 应用某个评价准则,从假设空间中选取一个最优的模型,使它对已知训练数据及未知测试数据在给定的评价准则下有最优的预测.
  - 最优模型的选取由算法实现.

- 实现统计学习方法的步骤:
  1. 得到一个有限的训练数据集合;
  2. 确定包含所有可能的模型的假设空间,即学习模型的集合;
  3. 确定模型选择的准则,即学习的策略;
  4. 实现求解最优模型的算法,即学习的算法;
  5. 通过学习方法,选择最优模型;
  6. 利用学习的最优模型对新数据进行预测或分析.

- 输入空间、特征空间与输出空间

  在监督学习中,将输入与输出所有可能取值的集合分别称为输入空间与输出空间.输入与输出空间可以是有限元素的集合,也可以是整个欧式空间.输入空间与输出空间可以是同一个空间,也可以是不用的空间;但通常输出空间远远小于输入空间.

  所有特征向量存在的空间称为特征空间.特征空间的每一维对应于一个特征.

- 假设空间

  通常一个模型是指由输入空间到输出空间映射的集合,而这个集合一般就是假设空间.假设空间的确定意味着学习范围的确定.

- 损失函数和风险函数

  损失函数是用来估计模型的预测值与真实值不一致的程度,损失函数越小,模型的鲁棒性就越好. 常见的损失函数有0-1损失函数、平方损失函数、绝对损失函数和对数损失函数.

- 经验风险最小化与结构风险最小化

  经验风险最小的模型是最优模型.当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。例如，极大似然估计（MLE）就是经验风险最小化的一个例子。当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等于极大似然估计。 

  但是，当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生过拟合现象。而结构风险最小化（structural risk minimization, SRM）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化。结构风险在经验风险的基础上加上表示模型复杂度的正则化项。

  结构风险小的模型往往对训练数据和未知的测试数据都有较好的预测。比如，贝叶斯估计中的最大后验概率估计（MAP）就是结构风险最小化的例子。当模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。 

  结构风险最小化的策略认为结构风险最小的模型是最优的模型

- 训练误差和测试误差 

  训练误差是模型Y关于训练数据集的平均损失。	

  测试误差是模型Y关于测试误差即的平均损失，反映了学习方法对未知的测试数据集的预测能力。

  测试误差=1-测试数据集上的准确率。

- 过拟合

  过拟合是指学习时选择的模型所包含的参数过多,以致于出现这一模型对已知数据预测得很好,但对未知数据预测得很差的现象.

- 正则化

  正则化一般用来解决结构性风险，过拟合问题.

  一般回归分析中回归ww表示特征的系数，从上式可以看到正则化项是对系数做了处理（限制）。L1正则化和L2正则化的说明如下： 

  ```
  L1正则化是指权值向量ww中各个元素的绝对值之和，通常表示为||w||1||w||1 
  L2正则化是指权值向量ww中各个元素的平方和然后再求平方根（可以看到Ridge回归的L2正则化项有平方符号），通常表示为||w||2||w||2 
  ```

  一般都会在正则化项之前添加一个系数，Python中用αα表示，一些文章也用λλ表示。这个系数需要用户指定。 

  那添加L1和L2正则化有什么用？下面是L1正则化和L2正则化的作用，这些表述可以在很多文章中找到。 

  L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，可以用于特征选择 

  L2正则化可以防止模型过拟合（overfitting）；一定程度上，L1也可以防止过拟合 

- 交叉验证

  在许多实际应用中,数据时不充足的,为了选择好的模型,可以采用交叉验证方法.交叉验证的基本想法是重复地使用数据;把给定的数据进行切分,将切分的数据集组合为训练集与测试集,在此基础上反复地训练、测试以及模型选择.

- 泛化能力:

  学习方法的泛化能力是指由该方法学习到的模型对未知数据的预测能力,是学习方法本质上重要的性质.

  现实中,多通过测试误差来评价学习方法的泛化能力.

  

## 主要问题

1. 分类问题

   从数据中学习一个分类模型或分类决策函数,称为分类器.分类器对新的输入进行输出的预测,称为分类.

2. 标注问题

   标注问题的输入是一个观测序列,输出是一个标记序列或状态序列.标注问题的目的在于学习一个模型,使它能够对观测序列作为预测.

   常用的学习方法:隐马尔可夫模型、条件随机场.

   标注问题主要在信息抽取、自然语言处理等领域被广泛应用.

3. 回归问题

   回归用于预测输入变量和输出变量之间的关系.回归问题的学习等价于函数拟合:选择一条曲线使其很好地拟合已知数据且很好地预测未知数据.

   回归问题按照输入变量的个数,分为一元回归和多元回归.

   回归学习最常用的损失函数是平方损失函数.

## 感知机模型

![1](https://mo-xiaoxi.github.io/img/post/machinelearning/1.png)

- 感知机(perceptron)是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别。

- 感知机对应于输入空间中将实例划分为正负两类的分离超平面，属于判别模型。
- 感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用**梯度下降法**对损失函数进行极小化，求得感知机模型。
- 感知机学习算法具有简单，并且易于实现的特点，分为原始形式和对偶形式。
- 感知机预测是用学习得到的感知机模型对新的输入实例进行分类。它是神经网络与支持向量机的基础。

- 感知机的目的:**假设**训练数据集是线性可分的，感知机学习的目的是求得一个能够将训练集整实例点和负实例点完全正确分开的**分离超平面**。为了找出这样的超平面，即确定感知机模型参数w 和 b, 需要确定一个学习策略，即**定义(经验)损失函数并将损失函数极小化**。

- 感知机是根据输入实例的特征向量x对其进行二类分类的先行分类模型:f(x)=sign(wx+b)

  感知机模型对英语输入空间(特征空间)中的分类超平面wx+b=0

- 感知机学习的策略是极小化损失函数:

  ![\min_{w, b} L(w, b) = - \sum_{xi \in M} y_i(w \bullet x_i + b) \space\space\space\space\space (2.5)](https://www.zhihu.com/equation?tex=%5Cmin_%7Bw%2C+b%7D+L%28w%2C+b%29+%3D+-+%5Csum_%7Bxi+%5Cin+M%7D+y_i%28w+%5Cbullet+x_i+%2B+b%29+%5Cspace%5Cspace%5Cspace%5Cspace%5Cspace+%282.5%29)

  损失函数对应于误分类点到分离超平面的总距离.

- 感知机学习算法是基于随机提督下降法的对损失函数的最优算法,有原始形式和对偶形式.算法简单且易于实现.原始形式中,首先人意选取一个超平面,然后用梯度下降发不断极小化目标函数.在这个过程中,一次随机选取一个误分类点使其梯度下降.


## K近邻法

![2](https://mo-xiaoxi.github.io/img/post/machinelearning/2.png)



- k近邻算法是一种**基本分类和回归方法**。

- K近邻算法，即是给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例**最邻近**的K个实例，**这K个实例的多数属于某个类**，就把该输入实例分类到这个类中。（**这就类似于现实生活中少数服从多数的思想**）更规范来讲,给定一个训练数据集,对新的输入实例,在训练数据集中找到于该实例最邻近的k个实例,这k个实例的多数属于某个类,就把该输入实例分为这个类.

  ![3](https://mo-xiaoxi.github.io/img/post/machinelearning/3.png)

  如上图所示，有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。这也就是我们的目的，来了一个新的数据点，我要得到它的类别是什么？好的，下面我们根据k近邻的思想来给绿色圆点进行分类。

  - 如果K=3，绿色圆点的最邻近的3个点是2个红色小三角形和1个蓝色小正方形，**少数从属于多数，**基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。
  - 如果K=5，绿色圆点的最邻近的5个邻居是2个红色三角形和3个蓝色的正方形，**还是少数从属于多数，**基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。

  从上面例子我们可以看出，k近邻的算法思想非常的简单，也非常的容易理解，那么我们是不是就到此结束了，**该算法的原理我们也已经懂了，也知道怎么给新来的点如何进行归类，只要找到离它最近的k个实例，哪个类别最多即可。**

- 模型主要由三个基本要素决定:距离度量、k值的选择和分类决策规则.

- k值的选择会对k近邻法的结果产生重大影响.

  如果我们选取较小的k值，那么就会意味着我们的整体模型会变得复杂，容易发生过拟合

  如果选择较大的k值,就相当于用较大邻域中的训练实例进行预测.其优点是可以减少学习的估计误差,但缺点是学习的近似误差会增大.

- kd树,主要用于解决如何对训练数据进行快速k近邻搜索.

  具体细节参考https://zhuanlan.zhihu.com/p/23966698



## 朴素贝叶斯

![7](https://mo-xiaoxi.github.io/img/post/machinelearning/7.png)

- 朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法.对于给定的训练数据集,首先基于特征条件独立假设学习输入/输入的联合概率分布;然后,基于此模型,对给定的输入x,利用贝叶斯定理求出后验概率最大的输出y.
- 朴素贝叶斯最大的假设便是对条件概率分布作了条件独立性的假设.这是一个较强的假设,因为在现实生活中,一般条件两则都不会相互独立.这也是朴素贝叶斯之所以朴素的原因.
- 朴素贝叶斯法利用贝叶斯原理与学到的联合概率模型进行分类预测.





## 决策树

- 决策树是一种基本的分类与回归方法.
- 决策树学习包含3个基本步骤:特征选择、决策树的生成和决策树的修剪.
- 决策树学习的损失函数通常是正则化的极大似然函数.决策树学习的策略是以损失函数为目标函数的最小化.
- 决策树学习常用的算法有ID3、C4.5和CART

- 信息增益 

  在决策树算法的学习过程中，**信息增益是特征选择的一个重要指标**，它定义为一个特征能够为分类系统带来多少信息，带来的信息越多，说明该特征越重要，**相应的信息增益也就越大。**

  https://zhuanlan.zhihu.com/p/26596036

- ID3算法的和兴是在决策树各个节点上应用信息增益准则选择特征,递归地构建决策树.具体方法是:从根节点开始,对节点计算所有可能特征的信息增益,选择信息增益最大的特征作为节点的特征,由该特征的不同取值建立子节点;再对子节点递归地调用以上方法,构建决策树;直到所有特征的信息增益均很小或没有特征可以选择为止.最后得到一个决策树,.ID3 相当于用极大似然法进行概率模型的选择.

- C4.5 与ID3 相比,主要在生成的过程中,用信息增益比来选择特征

-  CART算法是一种二分递归分割技术，把当前样本划分为两个子样本，使得生成的每个非叶子结点都有两个分支，因此CART算法生成的决策树是结构简洁的二叉树。由于CART算法构成的是一个二叉树，它在每一步的决策时只能是“是”或者“否”，即使一个feature有多个取值，也是把数据分为两部分。在CART算法中主要分为两个步骤

  - 1）将样本递归划分进行建树过程
  - 2）用验证数据进行剪枝



## 逻辑斯蒂回归与最大熵模型

- 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。 注意，这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。
- 逻辑回归（Logistic Regression）与线性回归（Linear Regression）都是一种广义线性模型（generalized linear model）。逻辑回归假设因变量 y 服从伯努利分布，而线性回归假设因变量 y 服从高斯分布。 因此与线性回归有很多相同之处，去除Sigmoid映射函数的话，逻辑回归算法就是一个线性回归。可以说，逻辑回归是以线性回归为理论支持的，但是逻辑回归通过Sigmoid函数引入了非线性因素，因此可以轻松处理0/1分类问题。

- 最大熵原理是概率模型学习的一个准则.最大熵原理认为,学习概率模型时,在所有可能的概率模型中,熵最大的模型时最好的模型.通常用约束条件来确定概率模型的集合.

- 最大熵在解决二分类问题时就是逻辑回归，在解决多分类问题时就是多项逻辑回归。为了证明最大熵模型跟逻辑回归的关系，那么就要证明两者求出来的模型是一样的，即求出来的h(x)的形式应该是一致的。

- 逻辑斯谛回归模型、最大熵模型学习归结为以似然函数为目标函数的最优化问题，通常通过迭代算法求解，它是光滑的凸函数，因此多种最优化的方法都适用。

  常用的方法有：改进的迭代尺度法/梯度下降法/牛顿法/拟牛顿法



## 支持向量机

- 支持向量机是一种二类分类模型,它的基本模型是定义在特征空间上的间隔最大的线性分类起,间隔最大使它有别于感知机;通过加入核技巧,支持向量机可以称为一个非线性分类器.
- SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。
- 非线性SVM算法原理:对于输入空间中的非线性分类问题，可以通过非线性变换将它转化为某个维特征空间中的线性分类问题，在高维特征空间中学习线性支持向量机。由于在线性支持向量机学习的对偶问题里，目标函数和分类决策函数都**只涉及实例和实例之间的内积，所以不需要显式地指定非线性变换**，**而是用核函数替换当中的内积**。核函数表示，通过一个非线性转换后的两个实例间的内积。



## 提升方法

- boosting算法系列的基本思想:

  ![8](https://mo-xiaoxi.github.io/img/post/machinelearning/8.png)

  Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。　　

- 理论上任何学习器都可以用于Adaboost.但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。
- AdaBoost的策略:
  1. **提高那些被前一轮弱分类器错误分类的样本的权值，而降低那些被正确分类样本的权值。**这样一来，那些被分错的数据，在下一轮就会得到更大的关注。所以，分类问题被一系列的弱分类器“分而治之”
  2. 对弱分类器的组合，AdaBoost采取加权多数表决的方法。**即加大分类误差率小的弱分类器的权值，使其在表决中起较大作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。**

- 可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二类分类学习方法.

- Adaboost算法的优缺点:

  1. Adaboost的主要优点有：
     - Adaboost作为分类器时，分类精度很高
     - 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
     - 作为简单的二元分类器时，构造简单，结果可理解。
     - 不容易发生过拟合
  2. Adaboost的主要缺点有：

     - 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。



## EM算法

- EM算法是一种迭代算法.EM算法的每次迭代由两部组成:E步,求期望;M步,求极大.所以,也被称为期望极大算法,简称EM算法.
- 在[统计](https://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1)[计算](https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97)中，**最大期望（EM）算法**是在[概率模型](https://zh.wikipedia.org/wiki/%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B)中寻找[参数](https://zh.wikipedia.org/wiki/%E5%8F%82%E6%95%B0)[最大似然估计](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)或者[最大后验估计](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87)的[算法](https://zh.wikipedia.org/wiki/%E7%AE%97%E6%B3%95)，其中概率模型依赖于无法观测的[隐变量](https://zh.wikipedia.org/wiki/%E9%9A%90%E5%8F%98%E9%87%8F)。最大期望算法经常用在[机器学习](https://zh.wikipedia.org/wiki/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0)和[计算机视觉](https://zh.wikipedia.org/wiki/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89)的[数据聚类](https://zh.wikipedia.org/wiki/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB)（Data Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在E步上求得的[最大似然值](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)来计算参数的值。M步上找到的参数估计值被用于下一个E步计算中，这个过程不断交替进行。

- 求最大似然函数估计值的一般步骤：
  - （1）写出似然函数；
  - （2）对似然函数取对数，并整理；
  - （3）求导数，令导数为0，得到似然方程；
  - （4）解似然方程，得到的参数即为所求；
- EM算法是常用的估计参数隐变量的利器.**EM算法通过引入隐含变量,使用MLE（极大似然估计）进行迭代求解参数**。通常引入隐含变量后会有两个参数，EM算法首先会固定其中的第一个参数，然后使用MLE计算第二个变量值；接着通过固定第二个变量，再使用MLE估测第一个变量值，依次迭代，直至收敛到局部最优解。



## 隐马尔可夫模型

- **隐马尔可夫模型**（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。
- 隐马尔可夫模型是关于时序的概率模型,描述由一个隐藏的马尔可夫链随机生成不可观测的状态随机序列,再由各个状态生成一个观测而产生观察随机序列的过程,隐藏的马尔可夫链随机生成的状态的序列,称为状态序列;每个状态生成一个观测,而由此产生的观测的随机序列,称为观测序列.序列的每一个位置又可以看作是一个时刻.
- 隐马尔可夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定.



## 条件随机场

- 条件随机场是给定一组输入随机变量条件下,另一组输出随机变量的条件概率分布模型,其特点是假设输出随机变量构成马尔可夫随机场.条件随机场可以用于不同的预测问题.
- 概率无向图模型,又称为马尔可夫随机场,是一个可以由无向图表示的联合概率分布.无向图模型的优点在于其没有隐马尔可夫模型那样严格的独立性假设，同时克服了最大熵马尔可夫模型等判别式模型的标记偏置问题。
- **条件随机场（Conditional random field，CRF）**是条件概率分布模型 ```P(Y|X)``` ，表示的是给定一组输入随机变量 X 的条件下另**一组输出**随机变量 Y 的马尔可夫随机场，也就是说 CRF 的特点是假设输出随机变量构成马尔可夫随机场。

  

## 统计学习方法总结



![9](https://mo-xiaoxi.github.io/img/post/machinelearning/9.png)

- 适用问题

  分类问题是从实例的特征向量到类标记的预测问题；标注问题是从观测序列到标记序列(或状态序列)的预测问题。可以认为分类问题是标注问题的特殊情况。

  分类问题中可能的预测结果是二类或多类；而标注问题中可能的预测结果是所有的标记序列，其数目是指数级的。

  感知机、k近邻法、朴素贝叶斯法、决策树是简单的分类方法，具有模型直观、方法简单、实现容易等特点；

  逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，往往分类准确率更高；

  隐马尔可夫模型、条件随机场是主要的标注方法。通常条件随机场的标注准确率更高。

- 模型

  分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射.它们可以写成条件概率分布 ```P(Y|X)P(Y|X)``` 或决策函数 Y=f(X)Y=f(X) 的形式。前者表示给定输入条件下输出的**概率模型**，后者表示输入到输出的**非概率模型**。有的模型只是其中一种，有的模型可以看成2者兼有。

  **朴素贝叶斯法、隐马尔可夫模型是概率模型；感知机、k近邻法、支持向量机、提升方法是非概率模型；而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看作是概率模型，又可以看作是非概率模型**。

  直接学习条件概率分布``` P(Y|X)P(Y|X) ```或决策函数 ```Y=f(X)Y=f(X) ```的方法为**判别方法**，对应的模型是**判别模型**：感知机、k近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法。

  首先学习联合概率分布 P(X,Y)P(X,Y)，从而求得条件概率分布 ```P(Y|X)P(Y|X) ```的方法是**生成方法**，对应的模型是**生成模型**：朴素贝叶斯法、隐马尔可夫模型是生成方法。

  决策树是定义在一般的特征空间上的，可以含有连续变量或离散变量。感知机、支持向量机、k近邻法的特征空间是欧氏空间(更一般地，是希尔伯特空间)。提升方法的模型是弱分类器的线性组合，弱分类器的特征空间就是提升方法模型的特征空间。

  感知机模型是**线性模型**；而逻辑斯谛回归与最大熵模型、条件随机场是**对数线性模型**；k近邻法、决策树、支持向量机(包含核函数)、提升方法使用的是**非线性模型**。

- 学习策略

  在二类分类的监督学习中，支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用**合页损失函数、逻辑斯谛损失函数、指数损失函数**，分别写为

  ![10](https://mo-xiaoxi.github.io/img/post/machinelearning/10.png)

  这3种损失函数都是0-1损失函数的上界，具有相似的形状。

  ![11](https://mo-xiaoxi.github.io/img/post/machinelearning/11.png)

  从上图可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数(surrogateloas Punotion)表示分类的损失，定义经验风险或结构风险函数，实现二类分类学习任务。学习的策略是优化以下结构风险函数

  ![12](https://mo-xiaoxi.github.io/img/post/machinelearning/12.png)

  第1项为经验风险(经验损失)，第2项为正则化项，L为损失函数，J(f)为模型的复杂度。

  支持向量机用L2范数表示模型的复杂度。原始的逻辑斯谛回归与最大熵模型没有正则化项，可以给它们加上L2范数正则化项。提升方法没有显式的正则化项，通常通过早停止(early stopping)的方法达到正则化的效果。

  概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计。学习的策略是极小化对数似然损失或极小化正则化的对数似然损失。极大后验概率估计时，正则化项是先验概率的负对数。

  决策树学习的策略是正则化的极大似然估计，损失函数是对数似然损失，正则化项是决策树的复杂度。

  逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计(或正则化的极大似然估计)，又可以看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)。

  朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计，但这时模型含有隐变量。

- 学习算法

  朴素贝叶斯法与隐马尔可夫模型的监督学习，最优解即**极大似然估计值**，可以由概率计算公式直接计算。

  感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、拟牛顿法等**一般的无约束最优化问题的解法**。

  支持向量机学习，可以解凸二次规划的**对偶问题**。有序列最小最优化算法等方法。

  决策树学习是基于启发式算法的典型例子。可以认为特征选择、生成、剪枝是启发式地进行正则化的极大似然估计。

  提升方法利用学习的模型是**加法模型**、损失函数是指数损失函数的特点，启发式地从前向后逐步学习模型，以达到逼近优化目标函数的目的。

  EM算法是一种迭代的求解**含隐变量概率模型参数**的方法，它的收敛性可以保证，但是不能保证收敛到全局最优。

  支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是**凸优化问题**，全局最优解保证存在。而其他学习问题则不是凸优化问题。



参考:
--------------------- 

1. 李航,《统计学习方法》
2. https://www.jiqizhixin.com/articles/2018-07-23-7
3. https://zhuanlan.zhihu.com/p/30155870
4. https://zhuanlan.zhihu.com/p/25994179
5. https://zhuanlan.zhihu.com/p/26596036
6. https://chenrudan.github.io/blog/2016/01/09/logisticregression.html
7. https://www.cnblogs.com/pinard/p/6133937.html
8. https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E6%9C%9F%E6%9C%9B%E7%AE%97%E6%B3%95
9. http://www.csuldw.com/2015/12/02/2015-12-02-EM-algorithms/